{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import array_contains\n",
    "import re\n",
    "\n",
    "# sentiment analysis\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_loc = '/Users/christopherkindl/Desktop/twitter_output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Random Text Classifier').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run sentiment analysis\n",
    "def apply_blob(sentence):\n",
    "    temp = TextBlob(sentence).sentiment[0]\n",
    "    if temp == 0.0:\n",
    "        return 0.0 # Neutral\n",
    "    elif temp >= 0.0:\n",
    "        return 1.0 # Positive\n",
    "    else:\n",
    "        return 2.0 # Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input\n",
    "df_raw = spark.read.option(\"header\", True).csv(input_loc)\n",
    "\n",
    "# perform text cleaning\n",
    "\n",
    "### -------- CHANGE HERE ---------\n",
    "\n",
    "# tokenize text\n",
    "#tokenizer = Tokenizer(inputCol='tweet', outputCol='tweet_token')\n",
    "#df_tokens = tokenizer.transform(df_raw).select('id','tweet_token')\n",
    "\n",
    "# remove emojis\n",
    "\n",
    "# remove special characters\n",
    "\n",
    "# remove stop words\n",
    "#remover = StopWordsRemover(\n",
    "#    inputCol='tweet_token', outputCol='tweet_clean')\n",
    "#df_clean = remover.transform(\n",
    "#    df_tokens).select('id', 'tweet_clean')\n",
    "\n",
    "# assign sentiment function as user defined function\n",
    "sentiment = udf(apply_blob)\n",
    "\n",
    "# apply sentiment function to all tweets\n",
    "df_clean = df_raw.withColumn('sentiment', sentiment(df_raw['tweet']))\n",
    "\n",
    "#\n",
    "## function to check presence of good\n",
    "#df_out = df_clean.select('cid', array_contains(\n",
    "#    df_clean.review_clean, \"good\").alias('positive_review'))\n",
    "## parquet is a popular column storage format, we use it here\n",
    "#df_out.write.mode(\"overwrite\").parquet(output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run sentiment analysis\n",
    "def apply_blob(sentence):\n",
    "    temp = TextBlob(sentence).sentiment[0]\n",
    "    if temp == 0.0:\n",
    "        return 0.0 # Neutral\n",
    "    elif temp >= 0.0:\n",
    "        return 1.0 # Positive\n",
    "    else:\n",
    "        return 2.0 # Negative\n",
    "    \n",
    "\n",
    "def sentiment_analysis(input_loc, output_loc):\n",
    "\n",
    "    ## -------- CHANGE HERE ---------\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a function to do the the following steps:\n",
    "        1. clean input data\n",
    "        2. run sentiment analysis using Vader\n",
    "        3. write final output to a HDFS output\n",
    "    \"\"\"\n",
    "\n",
    "    # read input\n",
    "    df_raw = spark.read.option(\"header\", True).csv(input_loc)\n",
    "    # perform text cleaning\n",
    "\n",
    "    ## -------- CHANGE HERE ---------\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol='review_str', outputCol='review_token')\n",
    "    df_tokens = tokenizer.transform(df_raw).select('cid', 'review_token')\n",
    "\n",
    "    # Remove stop words\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol='review_token', outputCol='review_clean')\n",
    "    df_clean = remover.transform(\n",
    "        df_tokens).select('cid', 'review_clean')\n",
    "\n",
    "    # function to check presence of good\n",
    "    df_out = df_clean.select('cid', array_contains(\n",
    "        df_clean.review_clean, \"good\").alias('positive_review'))\n",
    "    # parquet is a popular column storage format, we use it here\n",
    "    df_out.write.mode(\"overwrite\").parquet(output_loc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', type=str,\n",
    "                        help='HDFS input', default='/twitter')\n",
    "    parser.add_argument('--output', type=str,\n",
    "                        help='HDFS output', default='/output')\n",
    "    args = parser.parse_args()\n",
    "    spark = SparkSession.builder.appName(\n",
    "        'Sentiment Analysis').getOrCreate()\n",
    "    sentiment_analysis(input_loc=args.input, output_loc=args.output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
