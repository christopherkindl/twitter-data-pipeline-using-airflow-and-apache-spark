{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_text_classifier(input_loc, output_loc):\n",
    "\n",
    "    ## -------- CHANGE HERE ---------\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a dummy function to show how to use spark, It is supposed to mock\n",
    "    the following steps\n",
    "        1. clean input data\n",
    "        2. use a pre-trained model to make prediction\n",
    "        3. write predictions to a HDFS output\n",
    "\n",
    "    Since this is meant as an example, we are going to skip building a model,\n",
    "    instead we are naively going to mark reviews having the text \"good\" as positive and\n",
    "    the rest as negative\n",
    "    \"\"\"\n",
    "\n",
    "    # read input\n",
    "    df_raw = spark.read.option(\"header\", True).csv(input_loc)\n",
    "    # perform text cleaning\n",
    "\n",
    "    ## -------- CHANGE HERE ---------\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(inputCol='review_str', outputCol='review_token')\n",
    "    df_tokens = tokenizer.transform(df_raw).select('cid', 'review_token')\n",
    "\n",
    "    # Remove stop words\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol='review_token', outputCol='review_clean')\n",
    "    df_clean = remover.transform(\n",
    "        df_tokens).select('cid', 'review_clean')\n",
    "\n",
    "    # function to check presence of good\n",
    "    df_out = df_clean.select('cid', array_contains(\n",
    "        df_clean.review_clean, \"good\").alias('positive_review'))\n",
    "    # parquet is a popular column storage format, we use it here\n",
    "    df_out.write.mode(\"overwrite\").parquet(output_loc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', type=str,\n",
    "                        help='HDFS input', default='/twitter')\n",
    "    parser.add_argument('--output', type=str,\n",
    "                        help='HDFS output', default='/output')\n",
    "    args = parser.parse_args()\n",
    "    spark = SparkSession.builder.appName(\n",
    "        'Random Text Classifier').getOrCreate()\n",
    "    random_text_classifier(input_loc=args.input, output_loc=args.output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
