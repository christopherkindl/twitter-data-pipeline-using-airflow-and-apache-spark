{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lower, col, regexp_replace, concat_ws, collect_list\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType\n",
    "\n",
    "# standard modules\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_raw = spark.read.option(\"header\", True).parquet(\"/Users/christopherkindl/Downloads/df_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I really love that shirt at Macy'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove hashtags, usertags and links\n",
    "x=\"@peter I really love that shirt at #Macy. https://t.co/GdddIJwkse\"\n",
    "' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase text and remove special characters \n",
    "df_raw_lw = df_raw.select(\"date\", \"station\", (lower(regexp_replace('tweets', \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\")).alias('tweets')))\n",
    "df_raw_rm = df_raw_lw.select((regexp_replace('tweets',' +', ' ').alias('tweets')), \"date\", \"station\")\n",
    "#df_lower = df_raw.select(\"date\", \"station\", (lower(col(\"tweets\")).alias('tweets')))\n",
    "\n",
    "# tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"tweets\", outputCol=\"tweets_token\")\n",
    "df_tokens = tokenizer.transform(df_res).select(\"tweets_token\", \"date\", \"station\")\n",
    "\n",
    "# remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tweets_token\", outputCol=\"tweets_sw_removed\")#, stopWords=stopwordList)\n",
    "df_sw = remover.transform(df_tokens).select(\"tweets_sw_removed\", \"date\", \"station\")\n",
    "\n",
    "# stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_sw.withColumn(\"tweets\", stemmer_udf(\"tweets_sw_removed\")).select(\"tweets\", \"date\", \"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create string from list of strings\n",
    "join_udf = udf(lambda x: \",\".join(x))\n",
    "df_clean = df_stemmed.withColumn(\"tweets\", join_udf(col(\"tweets\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------+\n",
      "|              tweets|               date|         station|\n",
      "+--------------------+-------------------+----------------+\n",
      "|,qlondon,without,...|2021-04-27 08:30:07|      Abbey Road|\n",
      "|,jumper,arthur,be...|2021-04-26 08:49:34|      Abbey Road|\n",
      "|walk,wild,side,lo...|2021-04-21 17:20:23|   Acton Central|\n",
      "|walk,wild,side,lo...|2021-04-21 17:20:23| Acton Main Line|\n",
      "|click,link,bio,se...|2021-04-27 13:33:20|      Acton Town|\n",
      "|recommend,anyon,j...|2021-04-25 16:43:15|      Acton Town|\n",
      "|your,look,work,lo...|2021-04-22 13:15:30|      Acton Town|\n",
      "|rt,midst,london,b...|2021-04-28 14:26:17|         Aldgate|\n",
      "|long,due,catchup,...|2021-04-28 14:05:31|         Aldgate|\n",
      "|im,brace,thunder,...|2021-04-28 13:00:25|         Aldgate|\n",
      "|natur,nail,hello,...|2021-04-28 11:17:59|         Aldgate|\n",
      "|blackbottom,rubbe...|2021-04-28 00:34:13|         Aldgate|\n",
      "|rt,midst,london,b...|2021-04-28 14:26:17|    Aldgate East|\n",
      "|long,due,catchup,...|2021-04-28 14:05:31|    Aldgate East|\n",
      "|natur,nail,hello,...|2021-04-28 11:17:59|    Aldgate East|\n",
      "|blackbottom,rubbe...|2021-04-28 00:34:13|    Aldgate East|\n",
      "|big,shoutout,lodi...|2021-04-27 23:42:37|    Aldgate East|\n",
      "|post,photo,london...|2021-04-28 15:29:02|Alexandra Palace|\n",
      "|,havent,even,got,...|2021-04-25 10:03:19|Alexandra Palace|\n",
      "|post,photo,east,l...|2021-04-23 23:03:53|      All Saints|\n",
      "+--------------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by station and join tweets together\n",
    "df_tmp = df_clean.select('tweets', 'station').groupby('station').agg(concat_ws(\" \", collect_list(\"tweets\")).alias(\"tweets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up bag of words computation\n",
    "bow = df_tmp.rdd\\\n",
    "    .filter(lambda x: x.tweets)\\\n",
    "    .map( lambda x: x.tweets.replace(',',' ').replace('.',' ').replace('-',' '))\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\n",
    "    \n",
    "# run bag of words\n",
    "bow_tmp = bow.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "# show top 5 words    \n",
    "bow_sorted = bow_tmp.takeOrdered(10,lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of topics into single string\n",
    "topics = \"\"\n",
    "\n",
    "for index in range(len(bow_sorted)):\n",
    "    if bow_sorted[index][0] != 'london':\n",
    "        topics += bow_sorted[index][0]\n",
    "        topics += \", \"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataFrame\n",
    "columns = [\"date\",\"topics\"]\n",
    "curr_date = datetime.now().strftime('%Y-%m-%d')\n",
    "data = [(curr_date, topics)]\n",
    "df_final = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.mode(\"overwrite\").csv(\"/Users/christopherkindl/Desktop/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
